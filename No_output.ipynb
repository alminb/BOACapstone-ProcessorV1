{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e93a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "start_day = date.today()\n",
    "end_date = start_day\n",
    "print(end_date)\n",
    "search_term = '#bankofamerica'\n",
    "from_date = '2021-12-01'\n",
    "os.system(f\"snscrape --since {from_date} twitter-search '{search_term} until:{end_date}' > result-tweets.txt\")\n",
    "if os.stat(\"result-tweets.txt\").st_size == 0:\n",
    "          counter = 0\n",
    "else: \n",
    "          df = pd.read_csv('result-tweets.txt', names=['link'])\n",
    "          counter = df.size\n",
    "          \n",
    "print('Number of Tweets : ' + str(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b526659",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_results = 5000\n",
    "extracted_tweets = \"snscrape --format '{content!r}'\"+ f\" --max-results {max_results} --since {from_date} twitter-search '{search_term} until:{end_date}' > extracted-tweets.txt\"\n",
    "os.system(extracted_tweets)\n",
    "if os.stat(\"extracted-tweets.txt\").st_size == 0:\n",
    "    print(\"No tweets found\")\n",
    "else: \n",
    "    df = pd.read_csv('extracted-tweets.txt', names=['content'])\n",
    "    for row in df['content'].iteritems():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop = stopwords.words('english')\n",
    "df['stopwords'] = df['content'].apply(lambda x: len([x for x in str(x).split() if x in stop]))\n",
    "df[['content','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba80fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].str.replace('[^\\w\\s]','')\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df['content'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "import re\n",
    "import string\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560059ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "df['content'] = df['content'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "# remove all emojis from df\n",
    "df['content'] = df['content'].apply(lambda x: remove_emoji(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8109d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "df['content'] = df.content.apply(round1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286fd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)\n",
    "df['content'] = df.content.apply(round2)\n",
    "w = wordcloud.WordCloud(background_color=\"white\")  \n",
    "\n",
    "def newsfunction(a):\n",
    "    res = str()\n",
    "    for i in a:\n",
    "        res +=','\n",
    "        res += i\n",
    "    return ''.join(res)\n",
    "\n",
    "freq = pd.Series(' '.join(df['content']).split()).value_counts()[:50]\n",
    "new_data = pd.DataFrame(freq)\n",
    "new_data['keywords'] = freq.index\n",
    "abc = newsfunction(new_data['keywords'])\n",
    "w.generate(abc)\n",
    "w.to_file(\"pywordcloud2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "STOPWORDS = stopwords.words('english')\n",
    "vect = TfidfVectorizer(stop_words=STOPWORDS,max_features=1000)\n",
    "vect_text = vect.fit_transform(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=30,\n",
    "learning_method='online',random_state=100,max_iter=20) \n",
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxi(x):\n",
    "    maximum = x[0]\n",
    "    for i in range (0,len(x)):\n",
    "        if x[i] >= maximum:\n",
    "            maximum = x[i]\n",
    "            index=i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f06acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vect.get_feature_names()\n",
    "topics=[]\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "     vocab_comp = zip(vocab, comp)\n",
    "     sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "     c, v =  zip(*sorted_words)\n",
    "     topics.append(c)\n",
    "     print(\" \")\n",
    "     print(\"Topic \"+str(i)+\": \")\n",
    "     for t in sorted_words:\n",
    "#             topics[i].append(t[0])\n",
    "            print(t[0],end=\" \")\n",
    "     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c854049",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_index=[]\n",
    "for i,topic in enumerate(lda_top):\n",
    "#     print(\"Document i: \")\n",
    "    print(\"tweet \",i,\": \",maxi(topic))\n",
    "    topic_index.append(maxi(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_index'] = topic_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69417e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X=df['content'].values\n",
    "y=df['topic_index'].values.reshape(-1,1)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb463635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=123,random_state=18,max_depth=300)\n",
    "forest.fit(X_train,y_train)\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_forest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
