{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f2f47944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-20\n",
      "Number of Tweets : 2714\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "start_day = date.today()\n",
    "end_date = start_day\n",
    "print(end_date)\n",
    "search_term = '#bankofamerica'\n",
    "from_date = '2021-12-01'\n",
    "os.system(f\"snscrape --since {from_date} twitter-search '{search_term} until:{end_date}' > result-tweets.txt\")\n",
    "if os.stat(\"result-tweets.txt\").st_size == 0:\n",
    "          counter = 0\n",
    "else: \n",
    "          df = pd.read_csv('result-tweets.txt', names=['link'])\n",
    "          counter = df.size\n",
    "          \n",
    "print('Number of Tweets : ' + str(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "a63886e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_results = 5000\n",
    "extracted_tweets = \"snscrape --format '{content!r}'\"+ f\" --max-results {max_results} --since {from_date} twitter-search '{search_term} until:{end_date}' > extracted-tweets.txt\"\n",
    "os.system(extracted_tweets)\n",
    "if os.stat(\"extracted-tweets.txt\").st_size == 0:\n",
    "    print(\"No tweets found\")\n",
    "else: \n",
    "    df = pd.read_csv('extracted-tweets.txt', names=['content'])\n",
    "#     for row in df['content'].iteritems():\n",
    "#          print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5de9f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/gaode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/gaode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/gaode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gaode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import wordcloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop = stopwords.words('english')\n",
    "df['stopwords'] = df['content'].apply(lambda x: len([x for x in str(x).split() if x in stop]))\n",
    "# df[['content','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4a63df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "# df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "120eaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/snwvkg952g599fptk4bdqm3r0000gn/T/ipykernel_90745/2232345864.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['content'] = df['content'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].str.replace('[^\\w\\s]','')\n",
    "# df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ad01b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# df['content'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f84e800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "import re\n",
    "import string\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d01d9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "df['content'] = df['content'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "68ac5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "# remove all emojis from df\n",
    "df['content'] = df['content'].apply(lambda x: remove_emoji(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5e243357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "df['content'] = df.content.apply(round1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "85f7bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)\n",
    "df['content'] = df.content.apply(round2)\n",
    "# df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a51f13d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7fbd258012b0>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = wordcloud.WordCloud(background_color=\"white\")  \n",
    "\n",
    "def newsfunction(a):\n",
    "    res = str()\n",
    "    for i in a:\n",
    "        res +=','\n",
    "        res += i\n",
    "    return ''.join(res)\n",
    "\n",
    "freq = pd.Series(' '.join(df['content']).split()).value_counts()[:50]\n",
    "new_data = pd.DataFrame(freq)\n",
    "new_data['keywords'] = freq.index\n",
    "abc = newsfunction(new_data['keywords'])\n",
    "w.generate(abc)\n",
    "w.to_file(\"pywordcloud2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ab47b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "STOPWORDS = stopwords.words('english')\n",
    "vect = TfidfVectorizer(stop_words=STOPWORDS,max_features=2000)\n",
    "vect_text = vect.fit_transform(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "22ba182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=30,\n",
    "learning_method='online',random_state=100,max_iter=20) \n",
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "9e4c41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxi(x):\n",
    "    maximum = x[0]\n",
    "    for i in range (0,len(x)):\n",
    "        if x[i] >= maximum:\n",
    "            maximum = x[i]\n",
    "            index=i\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "31b9e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top 10 words in each topic\n",
    "vocab = vect.get_feature_names()\n",
    "topics=[]\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "     vocab_comp = zip(vocab, comp)\n",
    "     sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "     c, v =  zip(*sorted_words)\n",
    "     topics.append(c)\n",
    "#      print(\" \")\n",
    "#      print(\"Topic \"+str(i)+\": \")\n",
    "#      for t in sorted_words:\n",
    "# #             topics[i].append(t[0])\n",
    "#             print(t[0],end=\" \")\n",
    "#      print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "2c7a11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the index of the topic list of each tweet.\n",
    "topic_index=[]\n",
    "for i,topic in enumerate(lda_top):\n",
    "#     print(\"tweet \",i,\": \",maxi(topic))\n",
    "    topic_index.append(maxi(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e210bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_index'] = topic_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1fe91129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X=df['content'].values\n",
    "y=df['topic_index'].values.reshape(-1,1)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "567b9172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/snwvkg952g599fptk4bdqm3r0000gn/T/ipykernel_90745/3929655047.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  forest.fit(X_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9277777777777778\n"
     ]
    }
   ],
   "source": [
    "#Random Forest for tweets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=123,random_state=18,max_depth=300)\n",
    "forest.fit(X_train,y_train)\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9ed818d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygooglenews import GoogleNews\n",
    "import newspaper\n",
    "\n",
    "gn = GoogleNews()\n",
    "newsArticles = gn.search(\"bank of america\")\n",
    "newsArticles = newsArticles['entries']\n",
    "articles = []\n",
    "for entry in newsArticles:\n",
    "    url = entry['link']\n",
    "    article = newspaper.Article(url=url, language='en')\n",
    "    article.download()\n",
    "    try:\n",
    "        article.parse();\n",
    "        if str(article.title)==\"Are you a robot?\":\n",
    "            continue\n",
    "        if str(article.text)==\"\":\n",
    "            continue\n",
    "        article = {\n",
    "            \"title\": str(article.title),\n",
    "            \"text\": str(article.text),\n",
    "            \"authors\": article.authors,\n",
    "            \"published_date\": str(article.publish_date),\n",
    "            \"top_image\": str(article.top_image),\n",
    "            \"videos\": article.movies,\n",
    "            \"keywords\": article.keywords,\n",
    "            \"summary\": str(article.summary)\n",
    "        }\n",
    "        #print(\"----------\"+article[\"title\"] + \"------:\" + article[\"text\"] + \"\\n\\n\")\n",
    "        articles.append(article);\n",
    "    except: #if url can not be parsed by parser go to next entry\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1468a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words=STOPWORDS,max_features=2000)\n",
    "vect_text = vect.fit_transform(dfa['text'])\n",
    "\n",
    "lda_model=LatentDirichletAllocation(n_components=10,\n",
    "learning_method='online',random_state=100,max_iter=20) \n",
    "lda_top=lda_model.fit_transform(vect_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "910c3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top 10 words in each topic\n",
    "vocab = vect.get_feature_names()\n",
    "topics=[]\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "     vocab_comp = zip(vocab, comp)\n",
    "     sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:5]\n",
    "     c, v =  zip(*sorted_words)\n",
    "     topics.append(c)\n",
    "#      print(\" \")\n",
    "#      print(\"Topic \"+str(i)+\": \")\n",
    "#      for t in sorted_words:\n",
    "# #             topics[i].append(t[0])\n",
    "#             print(t[0],end=\" \")\n",
    "#      print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e3f894ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the index of the topic list of each articel.\n",
    "topic_index=[]\n",
    "for i,topic in enumerate(lda_top):\n",
    "#     print(\"article \",i,\": \",maxi(topic))\n",
    "    topic_index.append(maxi(topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "9593b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa['topic_index'] = topic_index\n",
    "X=dfa['text'].values\n",
    "y=dfa['topic_index'].values.reshape(-1,1)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "# print(X.shape, y.shape)\n",
    "# print(X[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "c99b3e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaode/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "# predictions\n",
    "# # print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
    "# # print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
    "# # print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "# # print('R-squared Error:', metrics.r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5ddf18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))\n",
    "train_accuracy = clf.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "1e543d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8235294117647058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/snwvkg952g599fptk4bdqm3r0000gn/T/ipykernel_90745/3890746267.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  forest.fit(X_train,y_train)\n"
     ]
    }
   ],
   "source": [
    "#Random Forest for Articles\n",
    "forest = RandomForestClassifier(n_estimators=123,random_state=18,max_depth=300)\n",
    "forest.fit(X_train,y_train)\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_forest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
